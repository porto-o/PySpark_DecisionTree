{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904ab25c-5861-42bb-9221-fcdd30b3e5ee",
   "metadata": {},
   "source": [
    "# Algoritmo CART en PySpark\n",
    "## Escuela Superior de Cómputo - Instituto Politécnico Nacional\n",
    "\n",
    "### Equipo\n",
    "* Armas Ramírez Daniel\n",
    "* Porto García Ismael\n",
    "\n",
    "**Docente**:\n",
    "Miguel Sáncez Brito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df4bc78c-3169-458b-b0a1-e542ad1fa805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import sum, count, avg, collect_set\n",
    "from typing import List, Tuple, Dict\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import math\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DecisionTree\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "281754c8-bde1-4abe-9686-35aed7a2630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Función para rotar el dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        df(DataFrame): Dataframe al cual se le hará transpose    \n",
    "    Returns:\n",
    "        DataFrame: Un nuevo dataframe aplicando la rotación\n",
    "    \"\"\"\n",
    "    rows = df.collect()\n",
    "    row_1 = df.columns\n",
    "    n = len(row_1)\n",
    "    n_rows = len(rows)\n",
    "    data = [ [] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        data[i].append(row_1[i])\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n):\n",
    "            data[j].append(rows[i][j])\n",
    "    \n",
    "    new_cols = data[0]\n",
    "    new_rows = data[1:]\n",
    "    rotate_df = spark.createDataFrame(new_rows, new_cols)\n",
    "    \n",
    "    print(new_cols)\n",
    "    dcols = input(\"Selecciona las columnas a convertir en double, separadas por coma: \").strip()\n",
    "    dcols = [col.strip() for col in dcols.split(\",\") if col.strip() != \"\"]\n",
    "\n",
    "    icols = input(\"Selecciona las columnas a convertir en int, separadas por coma: \").strip()\n",
    "    icols = [col.strip() for col in icols.split(\",\") if col.strip() != \"\"]\n",
    "    \n",
    "    # convertir a flotante\n",
    "    for col in dcols:\n",
    "        rotate_df = rotate_df.withColumn(col, rotate_df[col].cast(DoubleType()))\n",
    "\n",
    "    #convertir a entero\n",
    "    for col in icols:\n",
    "        rotate_df = rotate_df.withColumn(col, rotate_df[col].cast(IntegerType()))\n",
    "    return rotate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "157c97cd-6ef1-47a8-830b-a7b75318f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(cols: List[str], df: DataFrame, colgroup: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Función para transponer el dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        cols(List[str]): Lsita de las columnas del DataFrame\n",
    "        df(DataFrame): Dataframe al cual se le hará transpose\n",
    "        colgroup(str): columna usada para agrupar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Una copia del dataframe aplicando Transpose\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    columns = df.agg(collect_set(colgroup)).collect()[0][0]\n",
    "    columns.insert(0, 'temp-col')\n",
    "    for col in cols:\n",
    "        if col == colgroup:\n",
    "            continue\n",
    "        count_v = df.groupBy(colgroup, col).agg(count(\"*\")).collect()\n",
    "        for item in count_v:\n",
    "            temp_row = [f\"{col}-{item[col]}\"]\n",
    "            for i in range(1, len(columns)):\n",
    "                if columns[i] == item[colgroup]:\n",
    "                    temp_row.append(item['count(1)'])\n",
    "                else:\n",
    "                    temp_row.append(0)\n",
    "            rows.append(temp_row)\n",
    "\n",
    "    transpose_df = spark.createDataFrame(rows, columns)\n",
    "    return transpose_df\n",
    "\n",
    "\n",
    "def read_csv(path:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Función para leer el archivo csv\n",
    "    Parameters:\n",
    "        path(str): path de ubicación del archivo csv\n",
    "    Returns:\n",
    "        List[DataFrame, List[str]]: retorna el DataFrame y la lista de las columnas\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = spark.read.csv(path, inferSchema=True, header=True)\n",
    "        cols = df.columns\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error leyendo el archivo')\n",
    "        print(str(e))\n",
    "    else:\n",
    "        return df, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "09546326-095b-47bd-97a8-8754d9637e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_vect(df: DataFrame) -> Dict[str, str]:\n",
    "    print(\"Columnas disponibles en el dataframe:\")\n",
    "    print(df.columns)\n",
    "    \n",
    "    target_col = input(\"Selecciona la columna a predecir (target): \").strip()\n",
    "    \n",
    "    feature_cols_input = input(\"Selecciona las columnas a usar como predictores, separadas por comas: \").strip()\n",
    "    predictors = [col.strip() for col in feature_cols_input.split(\",\") if col.strip() != \"\"]\n",
    "    \n",
    "    return predictors, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1da84157-7e17-41bf-9ad1-dd28c3ee5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def prepare_data(df, target_col, predictors):\n",
    "    data = df\n",
    "\n",
    "    if isinstance(df.schema[target_col].dataType, StringType):\n",
    "        si_target = StringIndexer(inputCol=target_col, outputCol=\"label\")\n",
    "        si_model_target = si_target.fit(data)\n",
    "        data = si_model_target.transform(data)\n",
    "        print(f\"\\nCodificación de '{target_col}':\")\n",
    "        for i, lbl in enumerate(si_model_target.labels):\n",
    "            print(f\"{i} -> {lbl}\")\n",
    "    else:\n",
    "        data = data.withColumnRenamed(target_col, \"label\")\n",
    "\n",
    "    feat_cols = []\n",
    "    for col_name in predictors:\n",
    "        if col_name == target_col or col_name.lower() == \"id\":\n",
    "            continue\n",
    "\n",
    "        if isinstance(df.schema[col_name].dataType, StringType):\n",
    "            si_feat = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_idx\")\n",
    "            si_model_feat = si_feat.fit(data)\n",
    "            data = si_model_feat.transform(data)\n",
    "            feat_cols.append(f\"{col_name}_idx\")\n",
    "            print(f\"\\nCodificación de '{col_name}':\")\n",
    "            for i, lbl in enumerate(si_model_feat.labels):\n",
    "                print(f\"{i} -> {lbl}\")\n",
    "        else:\n",
    "            feat_cols.append(col_name)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "    data = assembler.transform(data)\n",
    "\n",
    "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    print(\"\\nMuestra de train_data:\")\n",
    "    train_data.show(5, truncate=False)\n",
    "\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "43abd01a-abd0-4a49-8d69-f468b28ceab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def fit_model(train_data, test_data):\n",
    "\n",
    "    max_depth = float(input(\"Ingresa max_depth a usar: \"))\n",
    "    min_instances = float(input(\"Ingresa min_instances a usar: \"))\n",
    "    \n",
    "    dt = DecisionTreeClassifier(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"features\",\n",
    "        maxDepth=max_depth,\n",
    "        minInstancesPerNode=min_instances\n",
    "    )\n",
    "\n",
    "    # Entrenar\n",
    "    model = dt.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Accuracy\n",
    "    correct = predictions.filter(col(\"prediction\") == col(\"label\")).count()\n",
    "    total = predictions.count()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    # Falsos positivos por clase\n",
    "    labels = [row[0] for row in predictions.select(\"label\").distinct().collect()]\n",
    "    print(\"Falsos Positivos por clase:\")\n",
    "    for l in labels:\n",
    "        fp = predictions.filter((col(\"prediction\") == l) & (col(\"label\") != l)).count()\n",
    "        print(f\"Clase {l}: {fp}\")\n",
    "\n",
    "    # arbol\n",
    "    print(\"\\nÁrbol de decisión:\")\n",
    "    print(model.toDebugString)\n",
    "\n",
    "    return model, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9401703a-dbd2-44c6-af8e-7238c3404ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def predecir(model):\n",
    "    while True:\n",
    "        input_str = input(\"Ingresa las características separadas por comas (ej: 0.0,0.2,3.5): \")\n",
    "        if input_str == \"\":\n",
    "            break\n",
    "        \n",
    "        features = [float(x.strip()) for x in input_str.split(\",\")]\n",
    "    \n",
    "        df = spark.createDataFrame([(Vectors.dense(features),)], [\"features\"])\n",
    "    \n",
    "        prediction = model.transform(df).collect()[0][\"prediction\"]\n",
    "        print(f\"Predicción del modelo: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de45b121-68b4-4f57-964a-b0b13564f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al uso del modelo Decision Tree\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Por favor provee el path de ubicación del archivo csv:  temp.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Muestra del Dataframe\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|         Id|          1|          2|          3|          4|          5|\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|SepalLength|        5.1|        4.9|        4.7|        4.6|        5.0|\n",
      "| SepalWidth|        3.5|        3.0|        3.2|        3.1|        3.6|\n",
      "|PetalLength|        1.4|        1.4|        1.3|        1.5|        1.4|\n",
      "| PetalWidth|        0.2|        0.2|        0.2|        0.2|        0.2|\n",
      "|    Species|Iris-setosa|Iris-setosa|Iris-setosa|Iris-setosa|Iris-setosa|\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Desea rotar el Dataframe:  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selecciona las columnas a convertir en double, separadas por coma:  SepalLength, SepalWidth, PetalLength, PetalWidth\n",
      "Selecciona las columnas a convertir en int, separadas por coma:  Id,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Muestra del Dataframe\n",
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "| Id|SepalLength|SepalWidth|PetalLength|PetalWidth|    Species|\n",
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "|  1|        5.1|       3.5|        1.4|       0.2|Iris-setosa|\n",
      "|  2|        4.9|       3.0|        1.4|       0.2|Iris-setosa|\n",
      "|  3|        4.7|       3.2|        1.3|       0.2|Iris-setosa|\n",
      "|  4|        4.6|       3.1|        1.5|       0.2|Iris-setosa|\n",
      "|  5|        5.0|       3.6|        1.4|       0.2|Iris-setosa|\n",
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "\n",
      "Columnas disponibles en el dataframe:\n",
      "['Id', 'SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selecciona la columna a predecir (target):  Species\n",
      "Selecciona las columnas a usar como predictores, separadas por comas:  SepalLength, SepalWidth, PetalLength, PetalWidth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Codificación de 'Species':\n",
      "0 -> Iris-setosa\n",
      "\n",
      "Muestra de train_data:\n",
      "+---+-----------+----------+-----------+----------+-----------+-----+-----------------+\n",
      "|Id |SepalLength|SepalWidth|PetalLength|PetalWidth|Species    |label|features         |\n",
      "+---+-----------+----------+-----------+----------+-----------+-----+-----------------+\n",
      "|2  |4.9        |3.0       |1.4        |0.2       |Iris-setosa|0.0  |[4.9,3.0,1.4,0.2]|\n",
      "|3  |4.7        |3.2       |1.3        |0.2       |Iris-setosa|0.0  |[4.7,3.2,1.3,0.2]|\n",
      "|4  |4.6        |3.1       |1.5        |0.2       |Iris-setosa|0.0  |[4.6,3.1,1.5,0.2]|\n",
      "|5  |5.0        |3.6       |1.4        |0.2       |Iris-setosa|0.0  |[5.0,3.6,1.4,0.2]|\n",
      "+---+-----------+----------+-----------+----------+-----------+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa max_depth a usar:  2\n",
      "Ingresa min_instances a usar:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n",
      "Falsos Positivos por clase:\n",
      "Clase 0.0: 0\n",
      "\n",
      "Árbol de decisión:\n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_1b508bc709a4, depth=0, numNodes=1, numClasses=1, numFeatures=4\n",
      "  Predict: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  4.0, 3.2, 2.4, 4.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 0.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  \n"
     ]
    }
   ],
   "source": [
    "print(\"Bienvenido al uso del modelo Decision Tree\")\n",
    "path = input(\"Por favor provee el path de ubicación del archivo csv: \")\n",
    "df, cols = read_csv(path)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Muestra del Dataframe\")\n",
    "df.show(5)\n",
    "\n",
    "rotate_in = input(\"Desea rotar el Dataframe: \")\n",
    "if rotate_in == 'yes':\n",
    "    df = rotate(df)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Muestra del Dataframe\")\n",
    "    df.show(5)\n",
    "\n",
    "predictors, target_col = get_prediction_vect(df)\n",
    "train_data, test_data = prepare_data(df, target_col, predictors)\n",
    "\n",
    "model, predictions = fit_model(train_data, test_data)\n",
    "predecir(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
